{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "GEzkHSBj7Kye"
   },
   "source": [
    "### NLP, 텍스트 분석\n",
    "\n",
    "- Natural Language Processing: 기계가 인간의 언어를 이해하고 해석하는 데 중점. 기계번역, 질의응답시스템\n",
    "- 텍스트 분석: 비정형 텍스트에서 의미있는 정보를 추출하는 것에 중점\n",
    "- NLP는 텍스트 분석을 향상하게 하는 기반기술\n",
    "- NLP와 텍스트 분석의 근간에는 머신러닝이 존재. 과거 언어적인 룰 기반 시스템에서 텍스트 데이터 기반으로 모델을 학습하고 예측\n",
    "- 텍스트 분석은 머신러닝, 언어 이해, 통계 등을 활용한 모델 수립, 정보 추출을 통해 인사이트 및 예측 분석 등의 분석 작업 수행\n",
    "  - 텍스트 분류: 신문기사 카테고리 분류, 스팸 메일 검출 프로그램, 지도학습\n",
    "  - 감성 분석: 감정/판단/믿음/의견/기분 등의 주관적 요소 분석. 소셜미디어 감정분석, 영화 리뷰, 여론조사 의견분석, 지도학습, 비지도학습\n",
    "  - 텍스트 요약: 텍스트 내에서 중요한 주제나 중심 사상을 추출. 토픽 모델링\n",
    "  - 텍스트 군집화와 유사도 측정: 비슷한 유형의 문서에 대해 군집화 수행. 비지도 학습\n",
    "\n",
    "Text 분석 수행 프로세스\n",
    "- 텍스트 정규화\n",
    "  - 클렌징, 토큰화, 필터링/스톱워드 제거/철자 수정, Stemming, Lemmatization\n",
    "- 피처 벡터화 변환\n",
    "  - Bag of Words: Count 기반, TF-IDF 기반\n",
    "  - Word2Vec\n",
    "- ML 모델 수립 및 학습/예측/평가\n",
    "\n",
    "텍스트 전처리 - 텍스트 정규화\n",
    "- 클렌징: 분석에 방해되는 불필요한 문자, 기호를 사전에 제거. HTML, XML 태그나 특정 기호\n",
    "- 토큰화: 문서에서 문장을 분리하는 문장 토큰화와, 문장에서 단어를 토큰으로 분리하는 단어 토큰화\n",
    "- 필터링/스톱워드 제거/철자 수정: 분석에 큰 의미가 없는 단어를 제거\n",
    "- Stemming, Lemmatization: 문법적 또는 의미적으로 변화하는 단어의 원형을 찾음\n",
    "  - Stemming은 원형 단어로 변환 시 일반적인 방법을 적용하거나 더 단순화된 방법을 적용(단어만 입력받아서 원형을 탐색함)\n",
    "  - Lemmatization이 Stemming보다 정교하며 의미론적인 기반에서 단어의 원형을 찾음(단어와 품사를 함께 입력해서 원형을 탐색)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 2236,
     "status": "ok",
     "timestamp": 1614740714518,
     "user": {
      "displayName": "정지윤",
      "photoUrl": "https://lh6.googleusercontent.com/-tInzaoPSLkc/AAAAAAAAAAI/AAAAAAAAAIE/25W4wFbeeq8/s64/photo.jpg",
      "userId": "05674157054886607737"
     },
     "user_tz": -540
    },
    "id": "JKYMy5ls6plO",
    "outputId": "7d1aa95a-3ada-4567-ff3a-bcb9a7a4f491"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
      "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 517,
     "status": "ok",
     "timestamp": 1614740999160,
     "user": {
      "displayName": "정지윤",
      "photoUrl": "https://lh6.googleusercontent.com/-tInzaoPSLkc/AAAAAAAAAAI/AAAAAAAAAIE/25W4wFbeeq8/s64/photo.jpg",
      "userId": "05674157054886607737"
     },
     "user_tz": -540
    },
    "id": "Fwg0x05vC-sq",
    "outputId": "c2462d7b-c37e-4526-c22c-ee9b6f7313a4"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['The Matrix is everywhere its all around us, here even in this room.', 'You can see it out your window or on your television.', 'You feel it when you go to work, or go to church or pay your taxes.']\n",
      "<class 'list'> 3\n"
     ]
    }
   ],
   "source": [
    "# 문장 토큰화(sent tokenize): 마침표, 개행문자(\\n), 정규표현식\n",
    "from nltk import sent_tokenize\n",
    "\n",
    "text_sample = 'The Matrix is everywhere its all around us, here even in this room. \\\n",
    "              You can see it out your window or on your television. \\\n",
    "              You feel it when you go to work, or go to church or pay your taxes.'\n",
    "sentences = sent_tokenize(text=text_sample)\n",
    "print(sentences)\n",
    "print(type(sentences), len(sentences))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 529,
     "status": "ok",
     "timestamp": 1614741165595,
     "user": {
      "displayName": "정지윤",
      "photoUrl": "https://lh6.googleusercontent.com/-tInzaoPSLkc/AAAAAAAAAAI/AAAAAAAAAIE/25W4wFbeeq8/s64/photo.jpg",
      "userId": "05674157054886607737"
     },
     "user_tz": -540
    },
    "id": "xJVfJZC8EEm7",
    "outputId": "c0352518-4e69-43c4-890e-881e32c94268"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['The', 'Matrix', 'is', 'everywhere', 'its', 'all', 'around', 'us', ',', 'here', 'even', 'in', 'this', 'room', '.']\n",
      "<class 'list'> 15\n"
     ]
    }
   ],
   "source": [
    "# 단어 토큰화(word_tokenize): 공백, 콤마, 마침표, 개행문자, 정규표현식\n",
    "from nltk import word_tokenize\n",
    "\n",
    "sentence = 'The Matrix is everywhere its all around us, here even in this room.'\n",
    "words = word_tokenize(sentence)\n",
    "print(words)\n",
    "print(type(words), len(words))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 581,
     "status": "ok",
     "timestamp": 1614741759175,
     "user": {
      "displayName": "정지윤",
      "photoUrl": "https://lh6.googleusercontent.com/-tInzaoPSLkc/AAAAAAAAAAI/AAAAAAAAAIE/25W4wFbeeq8/s64/photo.jpg",
      "userId": "05674157054886607737"
     },
     "user_tz": -540
    },
    "id": "B0NgE5gLEeo_",
    "outputId": "bef0042a-f340-4387-c42c-6f18ce65471a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['The', 'Matrix', 'is', 'everywhere', 'its', 'all', 'around', 'us', ',', 'here', 'even', 'in', 'this', 'room', '.'], ['You', 'can', 'see', 'it', 'out', 'your', 'window', 'or', 'on', 'your', 'television', '.'], ['You', 'feel', 'it', 'when', 'you', 'go', 'to', 'work', ',', 'or', 'go', 'to', 'church', 'or', 'pay', 'your', 'taxes', '.']]\n",
      "<class 'list'> 3\n"
     ]
    }
   ],
   "source": [
    "# 문서에 대해 모든 단어를 토큰화\n",
    "from nltk import word_tokenize, sent_tokenize\n",
    "\n",
    "def tokenize_text(text):\n",
    "  sentences = sent_tokenize(text) # 문장별 분리토큰\n",
    "  word_tokens = [word_tokenize(sentence) for sentence in sentences]\n",
    "  return word_tokens\n",
    "\n",
    "word_tokens = tokenize_text(text_sample)\n",
    "print(word_tokens)\n",
    "print(type(word_tokens),len(word_tokens))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 529,
     "status": "ok",
     "timestamp": 1614742032547,
     "user": {
      "displayName": "정지윤",
      "photoUrl": "https://lh6.googleusercontent.com/-tInzaoPSLkc/AAAAAAAAAAI/AAAAAAAAAIE/25W4wFbeeq8/s64/photo.jpg",
      "userId": "05674157054886607737"
     },
     "user_tz": -540
    },
    "id": "nU1c8mIWG5kN",
    "outputId": "ef9b2272-801e-406f-96f6-e8e8008777d5"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
      "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 6,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 스톱워드 제거: the, is, a, will와 같이 문맥적으로 큰 의미가 없는 단어를 제거\n",
    "import nltk\n",
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 556,
     "status": "ok",
     "timestamp": 1614742211291,
     "user": {
      "displayName": "정지윤",
      "photoUrl": "https://lh6.googleusercontent.com/-tInzaoPSLkc/AAAAAAAAAAI/AAAAAAAAAIE/25W4wFbeeq8/s64/photo.jpg",
      "userId": "05674157054886607737"
     },
     "user_tz": -540
    },
    "id": "x_EOCKU3IA5h",
    "outputId": "518a7cd0-8de3-47d9-b86c-7fa4fea909c7"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "179\n",
      "['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', \"you're\", \"you've\", \"you'll\", \"you'd\", 'your', 'yours', 'yourself', 'yourselves', 'he', 'him', 'his']\n"
     ]
    }
   ],
   "source": [
    "# NLTK english stopwords 갯수 확인\n",
    "print(len(nltk.corpus.stopwords.words('english')))\n",
    "print(nltk.corpus.stopwords.words('english')[:20])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 562,
     "status": "ok",
     "timestamp": 1614742600122,
     "user": {
      "displayName": "정지윤",
      "photoUrl": "https://lh6.googleusercontent.com/-tInzaoPSLkc/AAAAAAAAAAI/AAAAAAAAAIE/25W4wFbeeq8/s64/photo.jpg",
      "userId": "05674157054886607737"
     },
     "user_tz": -540
    },
    "id": "hlW11OIZIcWa",
    "outputId": "f501dc83-b122-46b6-d5a2-60bd35d96ca5"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['matrix', 'everywhere', 'around', 'us', ',', 'even', 'room', '.'], ['see', 'window', 'television', '.'], ['feel', 'go', 'work', ',', 'go', 'church', 'pay', 'taxes', '.']]\n"
     ]
    }
   ],
   "source": [
    "# stopwords 필터링을 통한 제거\n",
    "import nltk\n",
    "stopwords = nltk.corpus.stopwords.words('english')\n",
    "all_tokens = []\n",
    "for sentence in word_tokens:\n",
    "  filtered_words = []\n",
    "  for word in sentence:\n",
    "    word = word.lower()\n",
    "    if word not in stopwords:\n",
    "      filtered_words.append(word)\n",
    "  all_tokens.append(filtered_words)\n",
    "print(all_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 561,
     "status": "ok",
     "timestamp": 1614743390052,
     "user": {
      "displayName": "정지윤",
      "photoUrl": "https://lh6.googleusercontent.com/-tInzaoPSLkc/AAAAAAAAAAI/AAAAAAAAAIE/25W4wFbeeq8/s64/photo.jpg",
      "userId": "05674157054886607737"
     },
     "user_tz": -540
    },
    "id": "Ug2jb00QKI82",
    "outputId": "28543f55-f621-4668-bb1e-f462eff85f85"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "work work work\n",
      "amus amus amus\n",
      "fant fanciest\n"
     ]
    }
   ],
   "source": [
    "# 문법적 또는 의미적으로 변화하는 단어의 원형을 찾는 방법\n",
    "# Stemmer(LancasterStemmer)\n",
    "from nltk.stem import LancasterStemmer\n",
    "stemmer = LancasterStemmer()\n",
    "print(stemmer.stem('working'), stemmer.stem('works'), stemmer.stem('worked'))\n",
    "print(stemmer.stem('amusing'), stemmer.stem('amuses'), stemmer.stem('amused'))\n",
    "print(stemmer.stem('fancier'), stemmer.stem('fanciest'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 1033,
     "status": "ok",
     "timestamp": 1614743263234,
     "user": {
      "displayName": "정지윤",
      "photoUrl": "https://lh6.googleusercontent.com/-tInzaoPSLkc/AAAAAAAAAAI/AAAAAAAAAIE/25W4wFbeeq8/s64/photo.jpg",
      "userId": "05674157054886607737"
     },
     "user_tz": -540
    },
    "id": "Eflj0bIXMs0r",
    "outputId": "c3b0adc9-a3d0-4a78-95f5-65a4b7e2c7ef"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
      "[nltk_data]   Unzipping corpora/wordnet.zip.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 13,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    " import nltk\n",
    " nltk.download('wordnet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 570,
     "status": "ok",
     "timestamp": 1614743394685,
     "user": {
      "displayName": "정지윤",
      "photoUrl": "https://lh6.googleusercontent.com/-tInzaoPSLkc/AAAAAAAAAAI/AAAAAAAAAIE/25W4wFbeeq8/s64/photo.jpg",
      "userId": "05674157054886607737"
     },
     "user_tz": -540
    },
    "id": "kdWI8WGeLCYS",
    "outputId": "b9828722-4016-4203-9e51-d6c957c0c881"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "work work work\n",
      "amuse amuse amuse\n",
      "fancy fancy\n"
     ]
    }
   ],
   "source": [
    "# Lemmatization(WordNetLemmatizer): 정확한 원형 단어 추출을 위해 단어의 품사를 직접 입력\n",
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "\n",
    "lemma = WordNetLemmatizer()\n",
    "print(lemma.lemmatize('working','v'), lemma.lemmatize('works','v'), lemma.lemmatize('worked','v'))\n",
    "print(lemma.lemmatize('amusing','v'), lemma.lemmatize('amuses','v'), lemma.lemmatize('amused','v'))\n",
    "print(lemma.lemmatize('fancier','a'), lemma.lemmatize('fanciest','a'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WJWlHrx1lanL"
   },
   "source": [
    "GPU vs CPU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 9843,
     "status": "ok",
     "timestamp": 1614817634016,
     "user": {
      "displayName": "정지윤",
      "photoUrl": "https://lh6.googleusercontent.com/-tInzaoPSLkc/AAAAAAAAAAI/AAAAAAAAAIE/25W4wFbeeq8/s64/photo.jpg",
      "userId": "05674157054886607737"
     },
     "user_tz": -540
    },
    "id": "qC647QxEoQaZ",
    "outputId": "392956ea-ea96-4f56-89a1-55dcf22f7493"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3\n",
      "7/7 [==============================] - 7s 64ms/step - loss: 234.0989\n",
      "Epoch 2/3\n",
      "7/7 [==============================] - 0s 49ms/step - loss: 243.7826\n",
      "Epoch 3/3\n",
      "7/7 [==============================] - 0s 49ms/step - loss: 245.0283\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "num_samples = 100\n",
    "height = 71\n",
    "width = 71\n",
    "num_classes = 100\n",
    "\n",
    "import tensorflow as tf\n",
    "from keras.applications import Xception\n",
    "import datetime\n",
    "start = datetime.datetime.now()\n",
    "\n",
    "\n",
    "\n",
    "model = Xception(weights = None,\n",
    "                input_shape = (height, width, 3),\n",
    "                classes = num_classes)\n",
    "model.compile(loss='categorical_crossentropy',\n",
    "              optimizer = 'rmsprop')\n",
    "x = np.random.random((num_samples, height, width, 3))\n",
    "y = np.random.random((num_samples, num_classes))\n",
    "model.fit(x,y,epochs = 3, batch_size = 16)\n",
    "model.save('my_model.h5')\n",
    "end = datetime.datetime.now()\n",
    "time_delta = end - start"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 596,
     "status": "ok",
     "timestamp": 1614817636486,
     "user": {
      "displayName": "정지윤",
      "photoUrl": "https://lh6.googleusercontent.com/-tInzaoPSLkc/AAAAAAAAAAI/AAAAAAAAAIE/25W4wFbeeq8/s64/photo.jpg",
      "userId": "05674157054886607737"
     },
     "user_tz": -540
    },
    "id": "lEgOQqVtoQPy",
    "outputId": "0e65eb4f-ebef-4ec1-88fa-5cb1f3e4bf15"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "걸린 시간:9초\n"
     ]
    }
   ],
   "source": [
    "print('걸린 시간:{}초'.format(time_delta.seconds))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 38801,
     "status": "ok",
     "timestamp": 1614817575038,
     "user": {
      "displayName": "정지윤",
      "photoUrl": "https://lh6.googleusercontent.com/-tInzaoPSLkc/AAAAAAAAAAI/AAAAAAAAAIE/25W4wFbeeq8/s64/photo.jpg",
      "userId": "05674157054886607737"
     },
     "user_tz": -540
    },
    "id": "B_ZfAIIJlc3b",
    "outputId": "ce598fc5-b4e3-4f59-b9d5-be3b45d8ea01"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3\n",
      "7/7 [==============================] - 17s 1s/step - loss: 231.3479\n",
      "Epoch 2/3\n",
      "7/7 [==============================] - 10s 1s/step - loss: 239.0170\n",
      "Epoch 3/3\n",
      "7/7 [==============================] - 10s 1s/step - loss: 240.1834\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "num_samples = 100\n",
    "height = 71\n",
    "width = 71\n",
    "num_classes = 100\n",
    "\n",
    "import tensorflow as tf\n",
    "from keras.applications import Xception\n",
    "import datetime\n",
    "start = datetime.datetime.now()\n",
    "\n",
    "\n",
    "with tf.device('/cpu:0'):\n",
    "  model = Xception(weights = None,\n",
    "                  input_shape = (height, width, 3),\n",
    "                  classes = num_classes)\n",
    "  model.compile(loss='categorical_crossentropy',\n",
    "                optimizer = 'rmsprop')\n",
    "  x = np.random.random((num_samples, height, width, 3))\n",
    "  y = np.random.random((num_samples, num_classes))\n",
    "  model.fit(x,y,epochs = 3, batch_size = 16)\n",
    "  model.save('my_model.h5')\n",
    "end = datetime.datetime.now()\n",
    "time_delta = end - start"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 607,
     "status": "ok",
     "timestamp": 1614817581588,
     "user": {
      "displayName": "정지윤",
      "photoUrl": "https://lh6.googleusercontent.com/-tInzaoPSLkc/AAAAAAAAAAI/AAAAAAAAAIE/25W4wFbeeq8/s64/photo.jpg",
      "userId": "05674157054886607737"
     },
     "user_tz": -540
    },
    "id": "tXfu5Q-fnFwE",
    "outputId": "c09d120b-5fd3-4962-a77b-397763701a97"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "걸린 시간:38초\n"
     ]
    }
   ],
   "source": [
    "print('걸린 시간:{}초'.format(time_delta.seconds))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xBWtP9y4niVa"
   },
   "source": [
    "피처 벡터화: One-hot encoding\n",
    "Bag of Words: 문맥이나 순서를 무시하고 일괄적으로 단어에 대한 빈도값을 부여해 피처값을 추출하는 모델\n",
    "\n",
    "단점: 문맥 의미 반영 부족, 희소 행렬 문제,\n",
    "\n",
    "BOW에서 피처 벡터화: 모든 단어를 컬럼 형태로 나열하고 각 문서에서 해달 단어의 횟수나 정규화된 빈도를 값으로 부여하는 데이터 세트 모델로 변경하는 것.\n",
    "\n",
    "피처 벡터화 방식: 커운트 기반, TF-IDF(Term Frequency - Inverse Documnet Frequency) 기반 벡터화\n",
    "\n",
    "카운트 벡터화: 카운트값이 높을수록 중요한 단어로 인식. 특성상 자주 사용되는 보편적인 단어까지 높은 값 부여\n",
    "\n",
    "TF-IDF: 모든 문서에서 전반적으로 자주 나타나는 단어에 대해 패널티 부여. '빈번하게', '당연히', '조직', '업무' 등..\n",
    "\n",
    "파라미터\n",
    "\n",
    "- max_df: 너무 높은 빈도수를 가지는 단어 피처를 제외\n",
    "- min_df: 너무 낮은 빈도수를 가지는 단어 피처를 제외\n",
    "- max_features: 추출하는 피처의 개수를 제한하며 정수로 값을 지정\n",
    "- stop_words: 'english'로 지정하면 스톱워드로 지정된 단어는 추출에서 제외. 한글은 목록을 직접 작성해서 넘겨주는 수밖에 없음..\n",
    "- n_gram_range: 튜플 형태로 (범위 최솟값, 범위 최댓값)을 지정\n",
    "- analyzer: 피처 추출을 수행하는 단위. 디폴트는 'word'\n",
    "- token_pattern: 토큰화를 수행하는 정규 표현식 패턴 지정\n",
    "- tokenizer: 토큰화를 별도의 커스텀 함수로 이용시 적용"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[3, 0, 1],\n",
       "       [0, 2, 0]])"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# ndarray 객체 생성\n",
    "import numpy as np\n",
    "dense = np.array([[3,0,1],[0,2,0]])\n",
    "dense"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  (0, 0)\t3\n",
      "  (0, 2)\t1\n",
      "  (1, 1)\t2\n"
     ]
    }
   ],
   "source": [
    "# 희소행렬 - COO 형식\n",
    "from scipy import sparse\n",
    "data = np.array([3,1,2]) # 0이 아닌 데이터\n",
    "row_pos = np.array([0,0,1])\n",
    "col_pos = np.array([0,2,1])\n",
    "sparse_coo = sparse.coo_matrix((data, (row_pos, col_pos)))\n",
    "print(sparse_coo)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  (0, 2)\t1\n",
      "  (0, 5)\t5\n",
      "  (1, 0)\t1\n",
      "  (1, 1)\t4\n",
      "  (1, 3)\t3\n",
      "  (1, 4)\t2\n",
      "  (1, 5)\t5\n",
      "  (2, 1)\t6\n",
      "  (2, 3)\t3\n",
      "  (3, 0)\t2\n",
      "  (4, 3)\t7\n",
      "  (4, 5)\t8\n",
      "  (5, 0)\t1\n",
      "[[0 0 1 0 0 5]\n",
      " [1 4 0 3 2 5]\n",
      " [0 6 0 3 0 0]\n",
      " [2 0 0 0 0 0]\n",
      " [0 0 0 7 0 8]\n",
      " [1 0 0 0 0 0]]\n",
      "\n",
      "  (0, 2)\t1\n",
      "  (0, 5)\t5\n",
      "  (1, 0)\t1\n",
      "  (1, 1)\t4\n",
      "  (1, 3)\t3\n",
      "  (1, 4)\t2\n",
      "  (1, 5)\t5\n",
      "  (2, 1)\t6\n",
      "  (2, 3)\t3\n",
      "  (3, 0)\t2\n",
      "  (4, 3)\t7\n",
      "  (4, 5)\t8\n",
      "  (5, 0)\t1\n",
      "[[0 0 1 0 0 5]\n",
      " [1 4 0 3 2 5]\n",
      " [0 6 0 3 0 0]\n",
      " [2 0 0 0 0 0]\n",
      " [0 0 0 7 0 8]\n",
      " [1 0 0 0 0 0]]\n"
     ]
    }
   ],
   "source": [
    "# 희소행렬 - CSR 형식\n",
    "dense2 = np.array([[0,0,1,0,0,5],\n",
    "                  [1,4,0,3,2,5],\n",
    "                  [0,6,0,3,0,0],\n",
    "                  [2,0,0,0,0,0],\n",
    "                  [0,0,0,7,0,8],\n",
    "                  [1,0,0,0,0,0]])\n",
    "data2 = np.array([1,5,1,4,3,2,5,6,3,2,7,8,1])\n",
    "row_pos = np.array([0,0,1,1,1,1,1,2,2,3,4,4,5])\n",
    "col_pos = np.array([2,5,0,1,3,4,5,1,3,0,3,5,0])\n",
    "\n",
    "# COO 형식으로 변환\n",
    "sparse_coo = sparse.coo_matrix((data2,(row_pos, col_pos)))\n",
    "print(sparse_coo)\n",
    "print(sparse_coo.toarray())\n",
    "print()\n",
    "\n",
    "# CSR 형식으로 변환\n",
    "# 행 위치 배열의 고유한 값들의 시작 위치 인덱스를 배열로 생성\n",
    "# 끝에는 전체 개수인 13을 넣어준다.\n",
    "row_pos_ind = np.array([0,2,7,9,10,12,13])\n",
    "sparse_csr = sparse.csr_matrix((data2, col_pos, row_pos_ind))\n",
    "print(sparse_csr)\n",
    "print(sparse_csr.toarray())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1. 2. 0.]\n",
      " [0. 3. 1.]]\n",
      "['A', 'B', 'C']\n",
      "{'A': 0, 'B': 1, 'C': 2}\n",
      "[[0. 0. 4.]]\n"
     ]
    }
   ],
   "source": [
    "# DictVectorizer : 문서에서 단어의 사용빈도를 나타내는 딕셔너리 정보를 입력받아\n",
    "# BOW 인코딩한 수치 벡터로 전환\n",
    "from sklearn.feature_extraction import DictVectorizer\n",
    "v = DictVectorizer(sparse = False)\n",
    "D = [{'A':1,'B':2},{'B':3,'C':1}]\n",
    "X = v.fit_transform(D)\n",
    "print(X)\n",
    "print(v.feature_names_) # 어떤 Feature가 있는지 알려줌\n",
    "print(v.vocabulary_) # Feature의 순서 알려줌\n",
    "print(v.transform({'C':4,'D':3})) # fit이 없으면, 기존에 세팅되어있는 Dict에\n",
    "                            # 반영되기 때문에, C는 추가되지만 D는 추가되지 않음.\n",
    "                            # -> 말뭉치 사전"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['and', 'document', 'first', 'is', 'last', 'one', 'second', 'the', 'third', 'this']\n",
      "{'this': 9, 'is': 3, 'the': 7, 'first': 2, 'document': 1, 'second': 6, 'and': 0, 'third': 8, 'one': 5, 'last': 4}\n"
     ]
    }
   ],
   "source": [
    "# CountVectorizer\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "corpus = [\n",
    "    'This is the first document.',\n",
    "    'This is the second document.',\n",
    "    'And the third one',\n",
    "    'Is this the first document?',\n",
    "    'The last document?'\n",
    "]\n",
    "vect = CountVectorizer()\n",
    "vect.fit(corpus)\n",
    "print(vect.get_feature_names())\n",
    "print(vect.vocabulary_)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0 1 0 1 0 0 1 1 0 1]]\n"
     ]
    }
   ],
   "source": [
    "print(vect.transform(['This is the second document.']).toarray())\n",
    "# 위에 있는 vocabulary의 숫자에 맞춰 배열 인덱스 위치로 반환해 준 것."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0 0 0 0 0 0 0 0 0 0]]\n"
     ]
    }
   ],
   "source": [
    "print(vect.transform(['Something completely new.']).toarray())\n",
    "# 위에 있는 vocabulary의 목록에 단어들이 한 개도 포함되어 있지 않음."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0 1 1 1 0 0 0 1 0 1]\n",
      " [0 1 0 1 0 0 1 1 0 1]\n",
      " [1 0 0 0 0 1 0 1 1 0]\n",
      " [0 1 1 1 0 0 0 1 0 1]\n",
      " [0 1 0 0 1 0 0 1 0 0]]\n"
     ]
    }
   ],
   "source": [
    "print(vect.transform(corpus).toarray())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'first': 1, 'document': 0, 'second': 4, 'third': 5, 'one': 3, 'last': 2}"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Stop Words는 문서에서 단어장을 생성할 때 무시할 수 있는 단어.\n",
    "# 보통 영어의 관사, 접속사, 한국어의 조사 등\n",
    "\n",
    "vect = CountVectorizer(stop_words=['and','is','the','this']).fit(corpus)\n",
    "vect.vocabulary_\n",
    "# Stop Words가 빠지고 나머지 단어들만 남게 된다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'document': 0, 'second': 1}"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vect = CountVectorizer(stop_words='english').fit(corpus)\n",
    "vect.vocabulary_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'t': 16,\n",
       " 'h': 8,\n",
       " 'i': 9,\n",
       " 's': 15,\n",
       " ' ': 0,\n",
       " 'e': 6,\n",
       " 'f': 7,\n",
       " 'r': 14,\n",
       " 'd': 5,\n",
       " 'o': 13,\n",
       " 'c': 4,\n",
       " 'u': 17,\n",
       " 'm': 11,\n",
       " 'n': 12,\n",
       " '.': 1,\n",
       " 'a': 3,\n",
       " '?': 2,\n",
       " 'l': 10}"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# analyzer, tokenizer, token_pattern 등의 인수로 사용할 토큰 생성기를 선택\n",
    "# 문자 단위로 분해\n",
    "vect = CountVectorizer(analyzer=\"char\").fit(corpus)\n",
    "vect.vocabulary_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'this': 2, 'the': 0, 'third': 1}"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vect = CountVectorizer(token_pattern = 't\\w+').fit(corpus)\n",
    "vect.vocabulary_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'this': 20,\n",
       " 'is': 5,\n",
       " 'the': 13,\n",
       " 'first': 3,\n",
       " 'document': 2,\n",
       " 'this is': 21,\n",
       " 'is the': 6,\n",
       " 'the first': 14,\n",
       " 'first document': 4,\n",
       " 'second': 11,\n",
       " 'the second': 16,\n",
       " 'second document': 12,\n",
       " 'and': 0,\n",
       " 'third': 18,\n",
       " 'one': 10,\n",
       " 'and the': 1,\n",
       " 'the third': 17,\n",
       " 'third one': 19,\n",
       " 'is this': 7,\n",
       " 'this the': 22,\n",
       " 'last': 8,\n",
       " 'the last': 15,\n",
       " 'last document': 9}"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# n-그램은 단어장 생성에 사용할 토큰의 크기 결정\n",
    "vect = CountVectorizer(ngram_range=(1,2)).fit(corpus)\n",
    "vect.vocabulary_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'this': 3, 'the': 0, 'this the': 4, 'third': 2, 'the third': 1}"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vect = CountVectorizer(ngram_range=(1,2),token_pattern = 't\\w+').fit(corpus)\n",
    "vect.vocabulary_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TF-IDF(Term Frequency - Inverse Document Frequency) 인코딩은 단어를 갯수 그대로 카운트하지 않고, 모든 문서에 공통적으로 들어 있는 단어의 경우 문서 구별 능력이 떨어진다고 보아 가중치를 축소한다.\n",
    "\n",
    "문서 d(document)와 단어 t에 대해 다음와 같이 계산한다.\n",
    "\n",
    "tf-idf(d,t) = tf(d,t)*idf(t)\n",
    "\n",
    "- tf(d,t): term frequency. 특정한 단어의 빈도 수\n",
    "- idf(t): inverse document frequency. 특정한 단어가 들어 있는 문서의 수에 반비례하는 수\n",
    "- n: 전체 문서의 수\n",
    "- df(t): 단어 t를 가진 문서의 수\n",
    "- idf(d,t) = log(n/(1+df(t)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus = [\"\"\"\n",
    "The Corpus of Contemporary American English (COCA) is the only large, \n",
    "genre-balanced corpus of American English. COCA is probably the most \n",
    "widely-used corpus of English, and it is related to many other corpora of \n",
    "English that we have created, which offer unparalleled insight into variation \n",
    "in English.\n",
    "\"\"\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['american', 'balanced', 'coca', 'contemporary', 'corpora', 'corpus', 'created', 'english', 'genre', 'insight', 'large', 'offer', 'probably', 'related', 'unparalleled', 'used', 'variation', 'widely']\n",
      "\n",
      "{'corpus': 5, 'contemporary': 3, 'american': 0, 'english': 7, 'coca': 2, 'large': 10, 'genre': 8, 'balanced': 1, 'probably': 12, 'widely': 17, 'used': 15, 'related': 13, 'corpora': 4, 'created': 6, 'offer': 11, 'unparalleled': 14, 'insight': 9, 'variation': 16}\n",
      "\n",
      "[[0.26726124 0.13363062 0.26726124 0.13363062 0.13363062 0.40089186\n",
      "  0.13363062 0.6681531  0.13363062 0.13363062 0.13363062 0.13363062\n",
      "  0.13363062 0.13363062 0.13363062 0.13363062 0.13363062 0.13363062]]\n",
      "[[0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "tfidv = TfidfVectorizer(stop_words='english').fit(corpus)\n",
    "print(tfidv.get_feature_names())\n",
    "print()\n",
    "print(tfidv.vocabulary_)\n",
    "print()\n",
    "print(tfidv.transform(corpus).toarray())\n",
    "print(tfidv.transform(['This is the second document']).toarray())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "authorship_tag": "ABX9TyPx6jVM00DgmuhYaQPs/uKH",
   "collapsed_sections": [],
   "name": "nlp10_Text Mining.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
